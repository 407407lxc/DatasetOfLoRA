import os
import json
import re
import time
import argparse
from tqdm import tqdm
from openai import OpenAI
from vllm import LLM, SamplingParams

# --- å·¥å…·å‡½æ•° ---
def get_ref_letter(item):
    """
    é€šç”¨å‚è€ƒç­”æ¡ˆæå–
    ä¼ å…¥çš„æ˜¯æ•´ä¸ªå­—å…¸å¯¹è±¡ item
    """
    # å¦‚æœæ˜¯ MedMCQAï¼Œitem é‡Œé¢ä¼šæœ‰è¿™ä¸ª key
    if isinstance(item, dict) and 'answer_index' in item:
        mapping = {0: "A", 1: "B", 2: "C", 3: "D"}
        return mapping.get(item['answer_index'], "N/A")
    
    # é’ˆå¯¹ MathQAï¼Œitem é‡Œé¢ä¼šæœ‰ answer å­—æ®µ
    if isinstance(item, dict):
        return str(item.get('answer', '')).strip().upper()
    
    return "N/A"

def extract_answer(text):
    """å¤šçº§å®¹é”™æå–ï¼šå‡çº§æ­£åˆ™è¡¨è¾¾å¼æ”¯æŒ [A-E]"""
    # 1. è¿‡æ»¤æ‰æ€è€ƒè¿‡ç¨‹
    clean_text = text.split("</think>")[-1] if "</think>" in text else text
    
    # 2. å¼ºçº¦æŸæ ¼å¼ï¼šæ”¯æŒ A-E
    res1 = re.search(r"ANSWER:\s*([A-E])", clean_text, re.IGNORECASE)
    if res1: return res1.group(1).upper()
    
    # 3. ç»“è®ºå¥å¼
    res2 = re.search(r"correct\s*answer\s*is\s*([A-E])", clean_text, re.IGNORECASE)
    if res2: return res2.group(1).upper()
    
    # 4. å°¾éƒ¨æå– (é’ˆå¯¹ç”Ÿæˆè¾ƒé•¿ä¸”ä¸è§„èŒƒçš„æƒ…å†µ)
    res3 = re.findall(r"\b([A-E])\b", clean_text[-50:])
    if res3: return res3[-1].upper()
    
    return "ERR"


# --- MathQA è¯„æµ‹é€»è¾‘ ---
def run_mathqa_eval(args):
    print(f"ğŸ“¦ æ­£åœ¨åŠ è½½æ¨¡å‹: {args.model_path}")
    llm = LLM(model=args.model_path, trust_remote_code=True, gpu_memory_utilization=0.90)
    sampling_params = SamplingParams(
        temperature=0.0, 
        max_tokens=1024, 
        stop=["<|im_end|>", "Question:", "User:"]
    )

    raw_data, prompts = [], []
    with open(args.dataset_path, 'r', encoding='utf-8') as f:
        for line in f:
            item = json.loads(line.strip())
            raw_data.append(item)
            # MathQA é€šå¸¸æœ‰ A-E 5ä¸ªé€‰é¡¹
            labels = ['A', 'B', 'C', 'D', 'E']
            choices = item.get('choices', [])
            c_str = "\n".join([f"{labels[i]}. {choices[i]}" for i in range(len(choices)) if i < len(labels)])
            
            prompt = (
                f"Question: {item.get('question')}\nChoices:\n{c_str}\n\n"
                f"Assistant: <think>\nStep-by-step mathematical reasoning... Finally, provide the answer in 'ANSWER: X' format."
            )
            prompts.append(prompt)

    if args.sample_num:
        prompts = prompts[:args.sample_num]
        raw_data = raw_data[:args.sample_num]

    print(f"ğŸš€ å¼€å§‹ MathQA æµ‹è¯•æ¨ç† (æ ·æœ¬æ•°: {len(prompts)})...")
    start_time = time.time()
    outputs = llm.generate(prompts, sampling_params)
    total_duration = time.time() - start_time

    correct_count = 0
    results_to_save, debug_logs = [], []

    for i, output in enumerate(outputs):
        full_content = output.outputs[0].text.strip()
        ref_letter = get_ref_letter(raw_data[i]) # ä½¿ç”¨é€šç”¨æå–
        pred_letter = extract_answer(full_content) # ä½¿ç”¨é€šç”¨æå–
        
        is_correct = (pred_letter == ref_letter)
        if is_correct: correct_count += 1
        
        results_to_save.append({
            "id": i + 1, "is_correct": is_correct, "ref": ref_letter,
            "pred": pred_letter, "out_len": len(output.outputs[0].token_ids)
        })
        debug_logs.append({
            "id": i + 1, "question": raw_data[i].get('question'),
            "model_output": full_content, "ref": ref_letter, "pred": pred_letter
        })

    # ä¿å­˜åŒæ—¥å¿—
    with open(args.output_path, 'w', encoding='utf-8') as f:
        for res in results_to_save: f.write(json.dumps(res, ensure_ascii=False) + "\n")
    with open(args.debug_log_path, 'w', encoding='utf-8') as f:
        for log in debug_logs: f.write(json.dumps(log, ensure_ascii=False) + "\n")

    print(f"\n" + "="*25 + " MathQA è¯„ä¼°æŠ¥å‘Š " + "="*25)
    print(f"ğŸ“Š å‡†ç¡®ç‡: {correct_count / len(prompts):.2%}")
    print(f"â²ï¸ å¹³å‡é€Ÿåº¦: {len(prompts) / total_duration:.2f} samples/s")
    print(f"ğŸ“ å®Œæ•´ QA å·²å­˜å…¥: {args.debug_log_path}")
    print("="*64)

# --- MedMCQA è¯„æµ‹é€»è¾‘ (ä¿ç•™æœ¬åœ°ç¦»çº¿æ¨ç†ä¸åŒæ—¥å¿—è®°å½•) ---
def run_medmcqa_eval(args):
    print(f"ğŸ“¦ æ­£åœ¨åŠ è½½æ¨¡å‹: {args.model_path}")
    llm = LLM(model=args.model_path, trust_remote_code=True, gpu_memory_utilization=0.90)
    sampling_params = SamplingParams(
        temperature=0.0, 
        max_tokens=1024, 
        stop=["<|im_end|>", "Question:", "User:"]
    )

    raw_data, prompts = [], []
    with open(args.dataset_path, 'r', encoding='utf-8') as f:
        for line in f:
            item = json.loads(line.strip())
            raw_data.append(item)
            c_str = "\n".join([f"{['A','B','C','D'][i]}. {item['choices'][i]}" for i in range(len(item['choices']))])
            prompt = (
                f"Question: {item.get('question')}\nChoices:\n{c_str}\n\n"
                f"Assistant: <think>\nAnalyzing the medical evidence... Finally, provide the answer in 'ANSWER: X' format."
            )
            prompts.append(prompt)

    if args.sample_num:
        prompts = prompts[:args.sample_num]
        raw_data = raw_data[:args.sample_num]

    print(f"ğŸš€ å¼€å§‹ MedMCQA æµ‹è¯•æ¨ç† (æ ·æœ¬æ•°: {len(prompts)})...")
    start_time = time.time()
    outputs = llm.generate(prompts, sampling_params)
    total_duration = time.time() - start_time

    correct_count = 0
    results_to_save, debug_logs = [], []

    for i, output in enumerate(outputs):
        full_content = output.outputs[0].text.strip()
        ref_letter = get_ref_letter(raw_data[i])
        pred_letter = extract_answer(full_content)
        
        is_correct = (pred_letter == ref_letter)
        if is_correct: correct_count += 1
        
        results_to_save.append({
            "id": i + 1, "is_correct": is_correct, "ref": ref_letter,
            "pred": pred_letter, "out_len": len(output.outputs[0].token_ids)
        })
        debug_logs.append({
            "id": i + 1, "question": raw_data[i].get('question'),
            "model_output": full_content, "ref": ref_letter, "pred": pred_letter
        })

    # ä¿å­˜åŒæ—¥å¿—
    with open(args.output_path, 'w', encoding='utf-8') as f:
        for res in results_to_save: f.write(json.dumps(res, ensure_ascii=False) + "\n")
    with open(args.debug_log_path, 'w', encoding='utf-8') as f:
        for log in debug_logs: f.write(json.dumps(log, ensure_ascii=False) + "\n")

    print(f"\n" + "="*25 + " æœ€ç»ˆè¯„ä¼°æŠ¥å‘Š " + "="*25)
    print(f"ğŸ“Š å‡†ç¡®ç‡: {correct_count / len(prompts):.2%}")
    print(f"â²ï¸ å¹³å‡é€Ÿåº¦: {len(prompts) / total_duration:.2f} samples/s")
    print(f"ğŸ“ å®Œæ•´ QA å·²å­˜å…¥: {args.debug_log_path}")
    print("="*64)

# --- GSM8K è¯„æµ‹é€»è¾‘ (ä¿ç•™ API æ¨¡å¼ä¸å‰ 10 ä¾‹å®æ—¶ç›‘æ§) ---
import random
import concurrent.futures
from tqdm import tqdm
import json
import time

def run_gsm8k_eval(args):
    # 1. åŠ è½½æ¨¡å‹
    print(f"ğŸ“¦ æ­£åœ¨æœ¬åœ°åŠ è½½æ¨¡å‹è¿›è¡Œ GSM8K æµ‹è¯•: {args.model_path}")
    llm = LLM(model=args.model_path, trust_remote_code=True, gpu_memory_utilization=0.90)
    
    # ã€ä¼˜åŒ–ç‚¹ 1ã€‘å¢åŠ åœæ­¢ç¬¦ï¼Œé˜²æ­¢æ— é™å¤è¯»
    sampling_params = SamplingParams(
        temperature=0.0, 
        max_tokens=256, 
        # å¢åŠ å¤šç§åœæ­¢ç¬¦ï¼šé™¤äº† im_endï¼Œå¦‚æœè¾“å‡ºä¸¤ä¸ªæ¢è¡Œæˆ–æ£€æµ‹åˆ°é‡å¤è¾“å‡ºç‰¹å¾ä¹Ÿåœæ­¢
        stop=["Question:", "<|im_end|>", "\n\n\n", "#### 3\n#### 3"] 
    )
    
    # 2. å‡†å¤‡æ•°æ®
    with open(args.dataset_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()[:args.sample_num]
    
    questions, refs, prompts = [], [], []
    for line in lines:
        data = json.loads(line)
        q = data.get('query') or data.get('question')
        r = data.get('response') or data.get('answer')
        questions.append(q)
        refs.append(r)
        prompts.append(
            f"Instruction: Solve this math problem concisely within 256 tokens. "
            f"Directly provide steps and the final answer with ####.\n"
            f"Question: {q}\nAnswer: Let's calculate."
        )

    # 3. æ‰¹é‡æ¨ç†
    print(f"ğŸš€ å¼€å§‹æ‰¹é‡æ¨ç† (æ ·æœ¬æ•°: {len(prompts)})...")
    start_inference = time.time()
    outputs = llm.generate(prompts, sampling_params)
    total_duration = time.time() - start_inference
    
    predictions = [output.outputs[0].text.strip() for output in outputs]
    total_tokens = sum(len(output.outputs[0].token_ids) for output in outputs)
    avg_tps = total_tokens / total_duration if total_duration > 0 else 0

    # 4. å¥å£®çš„å¹¶è¡Œæ‰“åˆ†é€»è¾‘
    print(f"âš–ï¸ æ­£åœ¨å¹¶è¡Œæ‰“åˆ† (å« 429 é€€é¿æœºåˆ¶ä¸å†…å®¹æ¸…æ´—)...")
    client_judge = OpenAI(api_key=args.judge_key, base_url=args.judge_url)

    def get_judge_score(idx):
        q_item, ref_item, raw_pred = questions[idx], refs[idx], predictions[idx]
        
        # ã€ä¼˜åŒ–ç‚¹ 2ã€‘æ¸…æ´— Predictionï¼šæˆªæ–­å¤è¯»å†…å®¹
        # å¦‚æœæ¨¡å‹è¾“å‡ºäº†å¤šä¸ª ####ï¼Œåªä¿ç•™ç¬¬ä¸€ä¸ªåŠå…¶åçš„æ•°å­—éƒ¨åˆ†ï¼Œå¿½ç•¥åé¢çš„å¤è¯»
        clean_pred = raw_pred
        if "####" in raw_pred:
            parts = raw_pred.split("####")
            # ç»„åˆï¼š[è®¡ç®—è¿‡ç¨‹] + #### + [ç¬¬ä¸€ä¸ªæ•°å€¼]
            clean_pred = parts[0] + "#### " + parts[1].split("\n")[0].strip()

        max_retries = 5
        base_delay = 5 
        
        for attempt in range(max_retries):
            try:
                time.sleep(random.uniform(0.5, 1.5)) 
                # ä½¿ç”¨æ¸…æ´—åçš„ clean_pred å‘é€ç»™è£åˆ¤
                j_prompt = f"ä½ æ˜¯ä¸€åä¸¥æ ¼çš„æ•°å­¦è€å¸ˆã€‚\n[é—®é¢˜]: {q_item}\n[æ ‡å‡†ç­”æ¡ˆ]: {ref_item}\n[å­¦ç”Ÿå›ç­”]: {clean_pred}\n\nè¦æ±‚ï¼š\n1. å­¦ç”Ÿå¿…é¡»ç»™å‡ºæœ€ç»ˆæ•°å­—ç»“æœã€‚\n2. å¦‚æœå›ç­”åœ¨ä¸­é€”æ–­æ‰ï¼ˆå¦‚æ²¡å†™å®Œ #### åçš„æ•°å­—ï¼‰ï¼Œä¸€å¾‹åˆ¤ä¸ºâ€œé”™è¯¯â€ã€‚\n3. ä»…å½“æ•°å€¼ç»“æœä¸€è‡´æ—¶åˆ¤å®šä¸ºâ€œæ­£ç¡®â€ã€‚\n\nåªè¾“å‡ºæ­£ç¡®/é”™è¯¯"
                
                j_comp = client_judge.chat.completions.create(
                    model=args.judge_model,
                    messages=[{"role": "user", "content": j_prompt}],
                    temperature=0.0
                )
                return j_comp.choices[0].message.content.strip()
            except Exception as e:
                if "429" in str(e) and attempt < max_retries - 1:
                    wait_time = base_delay * (2 ** attempt)
                    time.sleep(wait_time)
                    continue
                return f"ERROR_JUDGE: {e}"

    # 5. æ‰§è¡Œæ‰“åˆ†
    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
        judge_results = list(tqdm(executor.map(get_judge_score, range(len(predictions))), total=len(predictions)))

    # 6. æ±‡æ€»ä¸è¾“å‡º (é€»è¾‘ä¿æŒä¸å˜)
    results = []
    correct_count = 0
    error_api_count = 0

    for i in range(len(lines)):
        res_text = judge_results[i]
        is_correct = "æ­£ç¡®" in res_text
        if is_correct: correct_count += 1
        if "ERROR_JUDGE" in res_text: error_api_count += 1
        
        if i < 10:
            print(f"\n{'='*20} Case {i+1} {'='*20}")
            print(f"Q: {questions[i]}\nRef: {refs[i]}\nPred: {predictions[i]}\nJudge: {res_text}")

        results.append({
            "id": i, "question": questions[i], "reference": refs[i], 
            "prediction": predictions[i], "judge_full": res_text, "is_correct": is_correct
        })

    with open(args.output_path, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=4)
    
    actual_evaluated = len(lines) - error_api_count
    print(f"\n" + "="*25 + " è¯„ä¼°ç»“æœ " + "="*25)
    print(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {len(lines)}")
    print(f"âœ… æ­£ç¡®æ•°é‡: {correct_count}")
    print(f"âŒ API å¤±è´¥: {error_api_count}")
    acc = correct_count/actual_evaluated if actual_evaluated > 0 else 0
    print(f"ğŸ“ˆ æœ‰æ•ˆå‡†ç¡®ç‡: {acc:.2%}")
    print(f"ğŸš€ æ¨ç†é€Ÿåº¦: {avg_tps:.2f} tokens/s")
    print("="*60)


# --- å‚æ•°è§£æ ---
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    # 1. åœ¨ choices ä¸­å¢åŠ  mathqa
    parser.add_argument("--task", type=str, required=True, choices=["gsm8k", "medmcqa", "mathqa"])
    parser.add_argument("--dataset_path", type=str, required=True)
    parser.add_argument("--output_path", type=str, required=True)
    parser.add_argument("--sample_num", type=int, default=None)
    
    # æœ¬åœ°æ¨ç†é€šç”¨å‚æ•° (MedMCQA & MathQA å…±ç”¨)
    parser.add_argument("--model_path", type=str, help="æœ¬åœ°æ¨¡å‹è·¯å¾„")
    parser.add_argument("--debug_log_path", type=str, default="debug.jsonl", help="è°ƒè¯•æ—¥å¿—è·¯å¾„")
    
    # GSM8K ä¸“ç”¨
    parser.add_argument("--vllm_url", type=str, default="http://127.0.0.1:8000/v1")
    parser.add_argument("--model_name", type=str)
    parser.add_argument("--judge_key", type=str)
    parser.add_argument("--judge_url", type=str, default="https://dashscope.aliyuncs.com/compatible-mode/v1")
    parser.add_argument("--judge_model", type=str, default="qwen2.5-72b-instruct")

    args = parser.parse_args()

    # 2. åˆ¤å®šé€»è¾‘åˆ†æ”¯
    if args.task == "gsm8k":
        run_gsm8k_eval(args)
    elif args.task == "medmcqa":
        run_medmcqa_eval(args)
    elif args.task == "mathqa":
        # æ–°å¢ mathqa è°ƒç”¨å…¥å£
        run_mathqa_eval(args)